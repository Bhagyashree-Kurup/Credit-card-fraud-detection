# -*- coding: utf-8 -*-
"""Credit card fraud with ANN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zjKIyDQka_uWDz3sfxjX90cPrwnY_L96
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Importing the dataset
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
dataset = pd.read_csv('/content/gdrive/MyDrive/creditcard fraud.csv')

# First 5 rows of the dataset
dataset.head()

# Last 5 rows of the dataset
dataset.tail()

# Understanding dataset informations
dataset.info()

# Visualizing the dataset 
sns.heatmap(dataset.corr())

fraudtime= dataset.groupby(['Time', 'Class']).Class.count().unstack()
fraudtime.plot(kind='line')

# Check the number of missing values in each column
dataset.isnull().sum()

# Understanding the distribution of legit transactions & fraudulent transactions
dataset['Class'].value_counts()

# Data separation into genuine and fraud for analysis
genuine = dataset[dataset.Class == 0]
fraud = dataset[dataset.Class == 1]

print(genuine.shape)
print(fraud.shape)

# Understanding the statistical measures of the genuine dataset
genuine.Amount.describe()

# Understanding the statistical measures of the fraud dataset
fraud.Amount.describe()

# Visualizing the transactions
import matplotlib.pyplot as plt
labels = ["Genuine", "Fraud"]
count_classes = dataset.value_counts(dataset['Class'], sort= True)
count_classes.plot(kind = "bar", rot = 0)
plt.title("Visualization of Transactions")
plt.ylabel("Count")
plt.xticks(range(2), labels)
plt.show()

# Compare the values for both transactions
dataset.groupby('Class').mean()

# Undersampling (as we can see from the visualization, the dataset is highly unbalanced, so to ensure accuracy and precision of our model and to overcome biasing, we are trying to make the data balanced)
genuine_sample = genuine.sample(n=492)

# Creating new balanced dataset by concatenating genuine_sample and fraud data
new_dataset = pd.concat([genuine_sample, fraud], axis=0)

# Feature Scaling the columns Amount and Time, as the rest columns are result of PCA
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()


new_dataset["Amount"] = sc.fit_transform(new_dataset["Amount"].values.reshape(-1, 1))
new_dataset["Time"] = sc.fit_transform(new_dataset["Time"].values.reshape(-1, 1))

# First 5 rows of the dataset
new_dataset.head()

# Last 5 rows of the dataset
new_dataset.tail()

new_dataset['Class'].value_counts()

# Compare the values for both the transactions
new_dataset.groupby('Class').mean()

X = new_dataset.drop(columns='Class', axis=1)
Y = new_dataset['Class']
X.info()

print (X)

print(Y)

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify=Y, random_state = 0)
print(X.shape, X_train.shape, X_test.shape)

# Lets make the ANN
# Importing the Keras libraries and packages
import keras
from keras.models import Sequential
from keras.layers import Dense

# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 30))

# Adding the second hidden layer
classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))

# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Fitting the ANN to the Training set
classifier.fit(X_train, Y_train, batch_size = 10, epochs = 100)

# Predicting the ANN for both train and test data
Y_train_pred = classifier.predict(X_train)
Y_train_pred = (Y_train_pred > 0.5)
Y_pred = classifier.predict(X_test)
Y_pred = (Y_pred > 0.5)

# Making the Confusion Matrix for test data
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_test, Y_pred)
print(cm)

# Visualizing the confusion matrix
import seaborn as sns
sns.heatmap(cm, annot=True)

#Performance metrics for both the training and testing data

# Accuracy 
from sklearn.metrics import accuracy_score
training_data_accuracy = accuracy_score(Y_train_pred, Y_train)
print('Accuracy on Training data is %f' % training_data_accuracy)
accuracy = accuracy_score(Y_test, Y_pred)
print('Accuracy on Test data is %f' % accuracy)

#Precision
from sklearn.metrics import precision_score
training_data_precision = precision_score(Y_train_pred, Y_train)
print('Precision on Training data is %f' % training_data_precision)
precision = precision_score(Y_test, Y_pred)
print('Precision on Test data is %f' % precision)
 
# Recall
from sklearn.metrics import recall_score
training_data_recall = recall_score(Y_train_pred, Y_train)
print('Recall on Training data is %f' % training_data_recall)
recall = recall_score(Y_test, Y_pred)
print('Recall on Test data is %f' % recall)

"""# New section"""